"""
RAG Workshop - Streamlit UI
============================
ì‹¤í–‰: streamlit run app.py

OECD í•œêµ­ ë””ì§€í„¸ ì •ë¶€ ë¦¬ë·° (2025) ë¬¸ì„œë¥¼ í™œìš©í•œ RAG ì‹¤ìŠµ
- 2025ë…„ ìµœì‹  ë¬¸ì„œë¡œ LLMì´ ì‚¬ì „ í•™ìŠµí•˜ì§€ ì•Šì€ ë‚´ìš©
- RAGì˜ ê°€ì¹˜ë¥¼ ëª…í™•íˆ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŒ
"""

import streamlit as st
import pandas as pd
from 3_RAG_playground_utilities import (
    Config, Document, Chunk,
    load_oecd_data, create_chunks, create_llm,
    SimpleVectorStore, RAGPipeline, cosine_similarity,
    OECD_SAMPLE_QA, keyword_search
)
import os

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RAG Workshop - OECD Korea Review",
    page_icon="ğŸ”",
    layout="wide"
)

st.title("ğŸ” RAG Workshop")
st.markdown("""
**OECD í•œêµ­ ë””ì§€í„¸ ì •ë¶€ ë¦¬ë·° (2025)** ê¸°ë°˜ RAG ì‹¤ìŠµ

ì´ ì›Œí¬ìƒµì—ì„œëŠ” 2025ë…„ 1ì›” ë°œí‘œëœ OECD ë¬¸ì„œë¥¼ í™œìš©í•©ë‹ˆë‹¤.
LLMì€ ì´ ë¬¸ì„œë¥¼ ì‚¬ì „ í•™ìŠµí•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—, RAG ì—†ì´ëŠ” ì •í™•í•œ ë‹µë³€ì´ ì–´ë µìŠµë‹ˆë‹¤.
""")

# ì‚¬ì´ë“œë°” - ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")

    # ì²­í¬ ì„¤ì •
    chunk_size = st.slider("ì²­í¬ í¬ê¸°", 700, 1500, 1000, 200)
    chunk_overlap = st.slider("ì²­í¬ ì˜¤ë²„ë©", 0, 200, 100, 50)
    top_k = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (Top-K)", 1, 10, 3)
    max_documents = st.slider("ë¡œë“œí•  ë¬¸ì„œ ìˆ˜", 10, 100, 50, 10)

    st.divider()

    # API ìƒíƒœ í‘œì‹œ
    st.header("ğŸ”‘ API ìƒíƒœ")
    google_key = os.getenv("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
    openai_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")

    if google_key:
        st.success("âœ… Gemini API ì‚¬ìš© ê°€ëŠ¥")
    if openai_key:
        st.success("âœ… OpenAI API ì‚¬ìš© ê°€ëŠ¥")
    if not google_key and not openai_key:
        st.error("âŒ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”")


# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "rag" not in st.session_state:
    st.session_state.rag = None
    st.session_state.llm = None  # LLM ê°ì²´ ì €ì¥ (ìˆœìˆ˜ API ë¹„êµìš©)
    st.session_state.documents = None
    st.session_state.chunks = None
    st.session_state.initialized = False


# ì´ˆê¸°í™” ë²„íŠ¼
col1, col2 = st.columns([1, 3])
with col1:
    if st.button("ğŸš€ RAG ì´ˆê¸°í™”", type="primary"):
        with st.spinner("ì´ˆê¸°í™” ì¤‘..."):
            try:
                # Config ì„¤ì •
                config = Config(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    top_k=top_k,
                    max_documents=max_documents
                )

                # ë°ì´í„° ë¡œë“œ (OECD ë¬¸ì„œ)
                st.info("ğŸ“¥ OECD ë°ì´í„° ë¡œë“œ ì¤‘...")
                documents = load_oecd_data(max_docs=config.max_documents)
                st.session_state.documents = documents

                # ì²­í‚¹
                st.info("âœ‚ï¸ ì²­í‚¹ ì¤‘...")
                chunks = create_chunks(documents, config.chunk_size, config.chunk_overlap)
                st.session_state.chunks = chunks

                # LLM ì´ˆê¸°í™”
                st.info("ğŸ¤– LLM ì´ˆê¸°í™” ì¤‘...")
                llm = create_llm(config)

                # ë²¡í„° ì €ì¥ì†Œ
                vector_store = SimpleVectorStore()

                # RAG íŒŒì´í”„ë¼ì¸
                rag = RAGPipeline(llm, vector_store, config)

                # ì¸ë±ì‹±
                progress_bar = st.progress(0)
                st.info("ğŸ”¢ ì„ë² ë”© ìƒì„± ì¤‘...")

                total = len(chunks)
                for i, chunk in enumerate(chunks):
                    chunk.embedding = llm.get_embedding(chunk.content)
                    progress_bar.progress((i + 1) / total)

                vector_store.add_chunks(chunks)

                st.session_state.rag = rag
                st.session_state.llm = llm  # LLM ì €ì¥
                st.session_state.initialized = True
                st.success(f"âœ… ì´ˆê¸°í™” ì™„ë£Œ! ({len(documents)}ê°œ ì±•í„°, {len(chunks)}ê°œ ì²­í¬)")

            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜: {e}")

with col2:
    if st.session_state.initialized:
        st.success(f"âœ… RAG ì¤€ë¹„ ì™„ë£Œ | {len(st.session_state.documents)}ê°œ ì±•í„° | {len(st.session_state.chunks)}ê°œ ì²­í¬")


# íƒ­ êµ¬ì„±
if st.session_state.initialized:
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ’¬ ì§ˆë¬¸í•˜ê¸°", "ğŸ“Š RAG vs API", "ğŸ”¤ ê²€ìƒ‰ ë°©ì‹", "ğŸ”¬ Top-K ì‹¤í—˜", "âœ‚ï¸ ì²­í‚¹ ì‹¤í—˜"])

    # íƒ­ 1: ììœ  ì§ˆë¬¸
    with tab1:
        st.subheader("ğŸ’¬ ììœ  ì§ˆë¬¸í•˜ê¸°")

        question = st.text_input(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ì˜ˆ: í•œêµ­ ë””ì§€í„¸ ì •ë¶€ì˜ ì£¼ìš” ê³¼ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
        )

        if st.button("ë‹µë³€ ìƒì„±", key="free_query"):
            if question:
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    result = st.session_state.rag.query(question)

                    st.markdown("### ğŸ“ ë‹µë³€")
                    st.write(result["answer"])

                    st.markdown("### ğŸ“š ì°¸ì¡° ë¬¸ì„œ")
                    for i, source in enumerate(result["sources"]):
                        with st.expander(f"{i+1}. {source['title']}"):
                            st.write(source["content"])
            else:
                st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

    # íƒ­ 2: ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ (RAG vs ìˆœìˆ˜ API ë¹„êµ)
    with tab2:
        st.subheader("ğŸ“Š RAG vs ìˆœìˆ˜ API ë¹„êµ")
        st.markdown("""
        **RAG ì‚¬ìš© ì‹œì™€ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œì˜ ë‹µë³€ì„ ë¹„êµí•©ë‹ˆë‹¤.**

        ğŸ’¡ *OECD í•œêµ­ ë””ì§€í„¸ ì •ë¶€ ë¦¬ë·°ëŠ” 2025ë…„ 1ì›” ë°œí‘œëœ ë¬¸ì„œì…ë‹ˆë‹¤.*
        *LLMì€ ì´ ë¬¸ì„œë¥¼ í•™ìŠµí•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—, RAG ì—†ì´ëŠ” ì •í™•í•œ ë‹µë³€ì´ ì–´ë µìŠµë‹ˆë‹¤.*
        """)

        # OECD ìƒ˜í”Œ ì§ˆë¬¸ ì‚¬ìš©
        sample_questions = OECD_SAMPLE_QA

        if sample_questions:
            selected = st.selectbox(
                "ìƒ˜í”Œ ì§ˆë¬¸ ì„ íƒ:",
                range(len(sample_questions)),
                format_func=lambda i: f"{sample_questions[i]['question'][:60]}..."
            )

            st.markdown(f"**ì •ë‹µ (Ground Truth):** `{sample_questions[selected]['answer']}`")

            if st.button("ğŸ”„ ë¹„êµ ì‹¤í–‰", key="compare_rag"):
                question = sample_questions[selected]["question"]
                ground_truth = sample_questions[selected]["answer"]

                col1, col2 = st.columns(2)

                # 1. ìˆœìˆ˜ API (RAG ì—†ì´)
                with col1:
                    st.markdown("### ğŸ’¬ ìˆœìˆ˜ API")
                    with st.spinner("ìˆœìˆ˜ API í˜¸ì¶œ ì¤‘..."):
                        pure_prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n\nì§ˆë¬¸: {question}\n\në‹µë³€:"
                        pure_answer = st.session_state.llm.generate(pure_prompt)
                        st.warning(pure_answer)

                        pure_correct = ground_truth.lower() in pure_answer.lower()
                        if pure_correct:
                            st.success("âœ… ì •ë‹µ í¬í•¨")
                        else:
                            st.error("âŒ ì •ë‹µ ë¯¸í¬í•¨")

                # 2. RAG ì‚¬ìš©
                with col2:
                    st.markdown("### ğŸ” RAG ì‚¬ìš©")
                    with st.spinner("RAG ë‹µë³€ ìƒì„± ì¤‘..."):
                        rag_result = st.session_state.rag.query(question)
                        st.success(rag_result["answer"])

                        rag_correct = ground_truth.lower() in rag_result["answer"].lower()
                        if rag_correct:
                            st.success("âœ… ì •ë‹µ í¬í•¨")
                        else:
                            st.error("âŒ ì •ë‹µ ë¯¸í¬í•¨")

                # ì°¸ì¡° ë¬¸ì„œ í‘œì‹œ
                st.markdown("---")
                st.markdown("### ğŸ“š RAGê°€ ì°¸ì¡°í•œ ë¬¸ì„œ")
                for i, source in enumerate(rag_result["sources"]):
                    with st.expander(f"{i+1}. {source['title']}"):
                        st.write(source["content"])

    # íƒ­ 3: ê²€ìƒ‰ ë°©ì‹ ë¹„êµ (Semantic vs Keyword)
    with tab3:
        st.subheader("ğŸ”¤ Semantic Search vs Keyword Search")
        st.markdown("""
        **ë‘ ê°€ì§€ ê²€ìƒ‰ ë°©ì‹ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.**

        | ë°©ì‹ | ì›ë¦¬ | íŠ¹ì§• |
        |------|------|------|
        | **Keyword (BM25)** | ë‹¨ì–´ ë¹ˆë„ + ì—­ë¬¸ì„œ ë¹ˆë„ | ë¹ ë¦„, ì •í™•í•œ ìš©ì–´ ë§¤ì¹­ |
        | **Semantic** | ì„ë² ë”© ë²¡í„° ìœ ì‚¬ë„ | ì˜ë¯¸ ì´í•´, ë™ì˜ì–´ ì²˜ë¦¬ |

        ğŸ’¡ *ì˜ˆ: "AIë²•"ì„ ê²€ìƒ‰í•˜ë©´ KeywordëŠ” ì •í™•íˆ "AIë²•"ì´ ìˆëŠ” ë¬¸ì„œë§Œ, Semanticì€ "ì¸ê³µì§€ëŠ¥ ë²•ë¥ "ë„ ì°¾ìŠµë‹ˆë‹¤.*
        """)

        st.divider()

        # ê²€ìƒ‰ ì§ˆë¬¸ ì…ë ¥
        search_query = st.text_input(
            "ê²€ìƒ‰í•  ì§ˆë¬¸:",
            value="í•œêµ­ì˜ ì¸ê³µì§€ëŠ¥ ê´€ë ¨ ë²•ë¥ ì€ ì–¸ì œ ì‹œí–‰ë˜ë‚˜ìš”?",
            key="search_compare_query"
        )

        col_k1, col_k2 = st.columns(2)
        with col_k1:
            search_top_k = st.slider("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (Top-K)", 1, 10, 5, key="search_topk")

        if st.button("ğŸ” ê²€ìƒ‰ ë¹„êµ ì‹¤í–‰", key="run_search_compare", type="primary"):
            if search_query:
                col_semantic, col_keyword = st.columns(2)

                # Semantic Search
                with col_semantic:
                    st.markdown("### ğŸ§  Semantic Search")
                    st.caption("ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ ìœ ì‚¬ë„ ê²€ìƒ‰")

                    with st.spinner("Semantic ê²€ìƒ‰ ì¤‘..."):
                        query_embedding = st.session_state.llm.get_embedding(search_query)
                        semantic_results = st.session_state.rag.vector_store.search(
                            query_embedding, top_k=search_top_k
                        )

                    for i, (chunk, score) in enumerate(semantic_results):
                        with st.expander(f"{i+1}. [{chunk.title}] (ìœ ì‚¬ë„: {score:.4f})"):
                            st.write(chunk.content[:300] + "...")

                # Keyword Search
                with col_keyword:
                    st.markdown("### ğŸ“ Keyword Search (BM25)")
                    st.caption("ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ ê²€ìƒ‰")

                    with st.spinner("Keyword ê²€ìƒ‰ ì¤‘..."):
                        keyword_results = keyword_search(
                            search_query,
                            st.session_state.chunks,
                            top_k=search_top_k
                        )

                    for i, (chunk, score) in enumerate(keyword_results):
                        with st.expander(f"{i+1}. [{chunk.title}] (BM25: {score:.4f})"):
                            st.write(chunk.content[:300] + "...")

                # ê²°ê³¼ ë¹„êµ ë¶„ì„
                st.divider()
                st.markdown("### ğŸ“Š ê²°ê³¼ ë¹„êµ ë¶„ì„")

                semantic_titles = [c.title for c, _ in semantic_results]
                keyword_titles = [c.title for c, _ in keyword_results]

                overlap = set(semantic_titles) & set(keyword_titles)
                only_semantic = set(semantic_titles) - set(keyword_titles)
                only_keyword = set(keyword_titles) - set(semantic_titles)

                col_stat1, col_stat2, col_stat3 = st.columns(3)
                col_stat1.metric("ê³µí†µ ê²°ê³¼", f"{len(overlap)}ê°œ")
                col_stat2.metric("Semanticë§Œ", f"{len(only_semantic)}ê°œ")
                col_stat3.metric("Keywordë§Œ", f"{len(only_keyword)}ê°œ")

                if only_semantic:
                    st.info(f"ğŸ§  Semanticë§Œ ì°¾ì€ ì±•í„°: {', '.join(only_semantic)}")
                if only_keyword:
                    st.info(f"ğŸ“ Keywordë§Œ ì°¾ì€ ì±•í„°: {', '.join(only_keyword)}")

    # íƒ­ 4: Top-K ë¹„êµ ì‹¤í—˜
    with tab4:
        st.subheader("ğŸ”¬ Top-K ë¹„êµ ì‹¤í—˜")

        test_question = st.text_input("ë¹„êµí•  ì§ˆë¬¸:", value=OECD_SAMPLE_QA[0]["question"] if OECD_SAMPLE_QA else "")

        k_values = st.multiselect("ë¹„êµí•  Top-K ê°’:", [1, 2, 3, 5, 7, 10], default=[1, 3, 5])

        if st.button("ë¹„êµ ì‹¤í–‰", key="compare"):
            if test_question and k_values:
                results = {}

                progress = st.progress(0)
                for i, k in enumerate(k_values):
                    with st.spinner(f"Top-{k} í…ŒìŠ¤íŠ¸ ì¤‘..."):
                        # ì„ì‹œë¡œ top_k ë³€ê²½
                        original_k = st.session_state.rag.config.top_k
                        st.session_state.rag.config.top_k = k

                        result = st.session_state.rag.query(test_question)
                        results[k] = result

                        st.session_state.rag.config.top_k = original_k
                        progress.progress((i + 1) / len(k_values))

                # ê²°ê³¼ í‘œì‹œ
                st.markdown("### ğŸ“Š ê²°ê³¼ ë¹„êµ")

                for k, result in results.items():
                    with st.expander(f"Top-{k} ê²°ê³¼"):
                        st.write(f"**ë‹µë³€:** {result['answer']}")
                        st.write(f"**ì°¸ì¡° ë¬¸ì„œ:** {[s['title'] for s in result['sources']]}")

    # íƒ­ 5: ì²­í‚¹ ì‹¤í—˜ (í† ì´ í”„ë¡œì íŠ¸)
    with tab5:
        st.subheader("âœ‚ï¸ ì²­í‚¹ ì‹¤í—˜ (Toy Project)")
        st.markdown("""
        **ì²­í¬ í¬ê¸°ì™€ ì˜¤ë²„ë©ì´ RAG ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹¤í—˜í•©ë‹ˆë‹¤.**
        - ì†Œê·œëª¨ ë°ì´í„° (3ê°œ ì±•í„°)ë¡œ ë¹ ë¥´ê²Œ ì‹¤í—˜
        - 3ê°€ì§€ ì§ˆë¬¸ìœ¼ë¡œ ê²°ê³¼ ë¹„êµ
        """)

        st.divider()

        # ì‹¤í—˜ìš© ë°ì´í„° ì¤€ë¹„ (3ê°œ ì±•í„°ë§Œ)
        toy_docs = st.session_state.documents[:3]

        # OECD ìƒ˜í”Œ ì§ˆë¬¸ 3ê°œ ì‚¬ìš©
        toy_questions = OECD_SAMPLE_QA[:3]

        if len(toy_questions) < 1:
            st.warning("ì§ˆë¬¸ì´ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ì‹¤í—˜ ì„¤ì •
            col_settings1, col_settings2 = st.columns(2)

            with col_settings1:
                st.markdown("### ğŸ“ ì²­í‚¹ ì„¤ì •")
                exp_chunk_sizes = st.multiselect(
                    "ì²­í¬ í¬ê¸° ì„ íƒ:",
                    [500, 700, 1000, 1200, 1500],
                    default=[500, 1000],
                    key="exp_chunk_size"
                )
                exp_overlap_ratio = st.slider(
                    "ì˜¤ë²„ë© ë¹„ìœ¨ (%):",
                    0, 50, 20, 5,
                    key="exp_overlap",
                    help="ì²­í¬ í¬ê¸°ì˜ ëª‡ %ë¥¼ ì˜¤ë²„ë©í• ì§€"
                )

            with col_settings2:
                st.markdown("### â“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸")
                for i, q in enumerate(toy_questions):
                    st.markdown(f"**Q{i+1}.** {q['question'][:50]}...")
                    st.caption(f"ì •ë‹µ: {q['answer']}")

            st.divider()

            # ì‹¤í—˜ ì‹¤í–‰
            if st.button("ğŸ§ª ì²­í‚¹ ì‹¤í—˜ ì‹¤í–‰", key="run_chunk_exp", type="primary"):
                if not exp_chunk_sizes:
                    st.warning("ì²­í¬ í¬ê¸°ë¥¼ 1ê°œ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
                else:
                    results_table = []

                    progress = st.progress(0)
                    total_steps = len(exp_chunk_sizes) * len(toy_questions)
                    current_step = 0

                    for chunk_size in exp_chunk_sizes:
                        overlap = int(chunk_size * exp_overlap_ratio / 100)

                        # ì²­í‚¹
                        exp_chunks = create_chunks(toy_docs, chunk_size=chunk_size, overlap=overlap)

                        # ì„ë² ë”© ìƒì„± (ì†Œê·œëª¨ë¼ ë¹ ë¦„)
                        with st.spinner(f"ì²­í¬ í¬ê¸° {chunk_size} ì„ë² ë”© ì¤‘..."):
                            for chunk in exp_chunks:
                                chunk.embedding = st.session_state.llm.get_embedding(chunk.content)

                        # ë²¡í„° ì €ì¥ì†Œ ë° RAG
                        exp_vector_store = SimpleVectorStore()
                        exp_vector_store.add_chunks(exp_chunks)

                        exp_config = Config(chunk_size=chunk_size, chunk_overlap=overlap, top_k=3)
                        exp_rag = RAGPipeline(st.session_state.llm, exp_vector_store, exp_config)

                        # ê° ì§ˆë¬¸ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
                        for q_idx, q in enumerate(toy_questions):
                            result = exp_rag.query(q["question"])
                            is_correct = q["answer"].lower() in result["answer"].lower()

                            results_table.append({
                                "ì²­í¬í¬ê¸°": chunk_size,
                                "ì˜¤ë²„ë©": overlap,
                                "ì²­í¬ìˆ˜": len(exp_chunks),
                                "ì§ˆë¬¸": f"Q{q_idx+1}",
                                "ì •ë‹µí¬í•¨": "âœ…" if is_correct else "âŒ",
                                "ì°¸ì¡°ë¬¸ì„œ": ", ".join([s["title"][:10] for s in result["sources"]])
                            })

                            current_step += 1
                            progress.progress(current_step / total_steps)

                    # ê²°ê³¼ í‘œì‹œ
                    st.markdown("### ğŸ“Š ì‹¤í—˜ ê²°ê³¼")

                    df = pd.DataFrame(results_table)
                    st.dataframe(df, use_container_width=True)

                    # ìš”ì•½
                    st.markdown("### ğŸ“ˆ ìš”ì•½")
                    for chunk_size in exp_chunk_sizes:
                        subset = [r for r in results_table if r["ì²­í¬í¬ê¸°"] == chunk_size]
                        correct_count = sum(1 for r in subset if r["ì •ë‹µí¬í•¨"] == "âœ…")
                        total_count = len(subset)
                        accuracy = correct_count / total_count * 100 if total_count > 0 else 0

                        chunk_count = subset[0]["ì²­í¬ìˆ˜"] if subset else 0

                        col1, col2, col3 = st.columns(3)
                        col1.metric(f"ì²­í¬ {chunk_size}", f"{chunk_count}ê°œ ì²­í¬")
                        col2.metric("ì •í™•ë„", f"{accuracy:.0f}%")
                        col3.metric("ì •ë‹µ", f"{correct_count}/{total_count}")

                    # ìƒì„¸ ê²°ê³¼
                    st.markdown("### ğŸ” ìƒì„¸ ê²°ê³¼")
                    for chunk_size in exp_chunk_sizes:
                        with st.expander(f"ì²­í¬ í¬ê¸°: {chunk_size}"):
                            subset = [r for r in results_table if r["ì²­í¬í¬ê¸°"] == chunk_size]
                            for r in subset:
                                status = r["ì •ë‹µí¬í•¨"]
                                st.markdown(f"{status} **{r['ì§ˆë¬¸']}** - ì°¸ì¡°: {r['ì°¸ì¡°ë¬¸ì„œ']}")

else:
    st.info("ğŸ‘† ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì • í›„ 'ğŸš€ RAG ì´ˆê¸°í™”' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")


# í‘¸í„°
st.divider()
st.caption("RAG Workshop | OECD Digital Government Review of Korea (2025) | Gemini / OpenAI")
