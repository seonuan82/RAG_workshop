"""
RAG ì±—ë´‡ - ì‹¬ë¦¬ ë‰´ìŠ¤ ê²€ìƒ‰ (ì •ë‹µ ë²„ì „)
======================================
ì‹¤í–‰: streamlit run rag_chatbot.py

ì‹¬ë¦¬ ê´€ë ¨ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” RAG ì±—ë´‡
"""

import streamlit as st
import pandas as pd
import numpy as np
import re
import math
import os
from pathlib import Path
from collections import Counter
from dataclasses import dataclass
from typing import Optional

# === í˜ì´ì§€ ì„¤ì • ===
st.set_page_config(
    page_title="RAG ì±—ë´‡ - ì‹¬ë¦¬ ë‰´ìŠ¤",
    page_icon="ğŸ“°",
    layout="wide"
)

# === ì„¤ì • ===
AVATAR_USER = "ğŸ‘¤"
AVATAR_BOT = "ğŸ¤–"
CSV_FILENAME = "Practice_data_NewsResult.CSV"
# GitHub Raw URL (ì €ì¥ì†Œì— CSV íŒŒì¼ ì—…ë¡œë“œ í›„ ì´ URL ìˆ˜ì • í•„ìš”)
GITHUB_CSV_URL = "https://raw.githubusercontent.com/seonuan82/RAG_Workshop/main/Practice_data_NewsResult.CSV"


def get_data_path():
    """ë°ì´í„° íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (ë¡œì»¬ > í˜„ì¬ ë””ë ‰í† ë¦¬ > None)"""
    # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ê²½ë¡œ
    try:
        current_dir = Path(__file__).parent
        local_path = current_dir / CSV_FILENAME
        if local_path.exists():
            return str(local_path)
    except:
        pass

    # Streamlit Cloudì—ì„œëŠ” í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ê¸°ì¤€
    cloud_path = Path(CSV_FILENAME)
    if cloud_path.exists():
        return str(cloud_path)

    # íŒŒì¼ì´ ì—†ìœ¼ë©´ None ë°˜í™˜ (GitHubì—ì„œ ë‹¤ìš´ë¡œë“œ í•„ìš”)
    return None


def load_news_from_github(max_items: int = 100) -> list:
    """GitHubì—ì„œ ë‰´ìŠ¤ CSV ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œë“œí•©ë‹ˆë‹¤."""
    import urllib.request
    import tempfile
    import io

    st.info("ğŸ“¥ GitHubì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")

    try:
        # URLì—ì„œ ì§ì ‘ ì½ê¸°
        with urllib.request.urlopen(GITHUB_CSV_URL) as response:
            content = response.read()

        # ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
        for encoding in ['cp949', 'euc-kr', 'utf-8', 'utf-8-sig']:
            try:
                decoded = content.decode(encoding)
                df = pd.read_csv(io.StringIO(decoded))
                st.success(f"âœ… GitHubì—ì„œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ (encoding: {encoding})")
                break
            except (UnicodeDecodeError, LookupError):
                continue
        else:
            decoded = content.decode('utf-8', errors='ignore')
            df = pd.read_csv(io.StringIO(decoded))

        return _parse_news_dataframe(df, max_items)

    except Exception as e:
        st.error(f"âŒ GitHub ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []


DATA_PATH = get_data_path()


# === ë°ì´í„° í´ë˜ìŠ¤ ===
@dataclass
class NewsItem:
    """ë‰´ìŠ¤ ë°ì´í„° í´ë˜ìŠ¤"""
    news_id: str
    date: str
    publisher: str
    title: str
    content: str
    url: str
    embedding: Optional[list] = None


# === ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ===
def init_session():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "news_data" not in st.session_state:
        st.session_state.news_data = []
    if "llm" not in st.session_state:
        st.session_state.llm = None
    if "embeddings_ready" not in st.session_state:
        st.session_state.embeddings_ready = False


init_session()


def reset_chat():
    """ëŒ€í™” ì´ˆê¸°í™”"""
    st.session_state.messages = []


def reset_all():
    """ì „ì²´ ì´ˆê¸°í™”"""
    st.session_state.messages = []
    st.session_state.news_data = []
    st.session_state.llm = None
    st.session_state.embeddings_ready = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _parse_news_dataframe(df: pd.DataFrame, max_items: int = 100) -> list:
    """DataFrameì„ NewsItem ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    news_list = []

    # ìµœëŒ€ max_itemsê°œë§Œ ì‚¬ìš©
    df = df.head(max_items)

    # ì»¬ëŸ¼ëª… í™•ì¸ ë° ë§¤í•‘ (ë‹¤ì–‘í•œ CSV í¬ë§· ì§€ì›)
    col_mapping = {
        'news_id': ['ë‰´ìŠ¤ ì‹ë³„ì', 'ê¸°ì‚¬ ê³ ìœ ë²ˆí˜¸', 'news_id', 'id'],
        'date': ['ì¼ì', 'date', 'ë‚ ì§œ'],
        'publisher': ['ì–¸ë¡ ì‚¬', 'publisher', 'ë§¤ì²´'],
        'title': ['ì œëª©', 'title'],
        'content': ['ë³¸ë¬¸', 'content', 'ë‚´ìš©'],
        'url': ['URL', 'url', 'ë§í¬']
    }

    def find_column(candidates):
        for col in candidates:
            if col in df.columns:
                return col
        return candidates[0]  # ê¸°ë³¸ê°’

    id_col = find_column(col_mapping['news_id'])
    date_col = find_column(col_mapping['date'])
    publisher_col = find_column(col_mapping['publisher'])
    title_col = find_column(col_mapping['title'])
    content_col = find_column(col_mapping['content'])
    url_col = find_column(col_mapping['url'])

    # ê° í–‰ì„ NewsItemìœ¼ë¡œ ë³€í™˜
    for _, row in df.iterrows():
        try:
            news = NewsItem(
                news_id=str(row.get(id_col, '')),
                date=str(row.get(date_col, '')),
                publisher=str(row.get(publisher_col, '')),
                title=str(row.get(title_col, '')),
                content=str(row.get(content_col, '')),  # ë³¸ë¬¸ ì „ì²´ ì €ì¥
                url=str(row.get(url_col, ''))
            )
            news_list.append(news)
        except Exception:
            continue

    return news_list


def load_news_data(filepath: Optional[str] = None, max_items: int = 100) -> list:
    """
    CSV íŒŒì¼ì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        filepath: CSV íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ë˜ëŠ” GitHubì—ì„œ ë‹¤ìš´ë¡œë“œ)
        max_items: ë¡œë“œí•  ìµœëŒ€ ë‰´ìŠ¤ ìˆ˜

    Returns:
        NewsItem ë¦¬ìŠ¤íŠ¸
    """
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    load_news_from_github(max_items)

    # ë¡œì»¬ íŒŒì¼ ë¡œë“œ
    st.info(f"ğŸ“‚ ë¡œì»¬ íŒŒì¼ì—ì„œ ë¡œë“œ: {filepath}")

    # CSV íŒŒì¼ ì½ê¸° (ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„)
    for encoding in ['cp949', 'euc-kr', 'utf-8', 'utf-8-sig']:
        try:
            df = pd.read_csv(filepath, encoding=encoding)
            st.success(f"âœ… íŒŒì¼ ë¡œë“œ ì™„ë£Œ (encoding: {encoding})")
            break
        except (UnicodeDecodeError, LookupError):
            continue
    else:
        # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ì˜¤ë¥˜ ë¬´ì‹œ
        df = pd.read_csv(filepath, encoding='utf-8', encoding_errors='ignore')
        st.warning("âš ï¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ (ì¼ë¶€ ë¬¸ì ì†ì‹¤ ê°€ëŠ¥)")

    return _parse_news_dataframe(df, max_items)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê´€ë ¨ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_relevant_news(query: str, news_data: list, top_k: int = 5) -> list:
    """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # BM25 ê²€ìƒ‰ ì‚¬ìš©
    results = bm25_search(query, news_data, top_k)
    return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë‰´ìŠ¤ ë°ì´í„° í¬ë§·íŒ…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def format_news_data(news_results: list) -> str:
    """ê²€ìƒ‰ëœ ë‰´ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    formatted_list = []

    for news, score in news_results:
        formatted = f"ì œëª©: {news.title}\nì–¸ë¡ ì‚¬: {news.publisher}\në‚ ì§œ: {news.date}\në³¸ë¬¸: {news.content}"
        formatted_list.append(formatted)

    return "\n\n---\n\n".join(formatted_list)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BM25 ê²€ìƒ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def tokenize(text: str) -> list:
    """ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì €"""
    text = text.lower()
    text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
    tokens = text.split()
    stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were',
                 'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'í•œ', 'ìˆë‹¤', 'í•˜ë‹¤'}
    return [t for t in tokens if t not in stopwords and len(t) > 1]


def bm25_score(query_tokens: list, doc_tokens: list,
               avg_doc_len: float, doc_count: int, doc_freqs: dict,
               k1: float = 1.5, b: float = 0.75) -> float:
    """BM25 ìŠ¤ì½”ì–´ ê³„ì‚°"""
    score = 0.0
    doc_len = len(doc_tokens)
    doc_token_counts = Counter(doc_tokens)

    for token in query_tokens:
        if token not in doc_token_counts:
            continue
        tf = doc_token_counts[token]
        df = doc_freqs.get(token, 0)
        idf = math.log((doc_count - df + 0.5) / (df + 0.5) + 1)
        numerator = tf * (k1 + 1)
        denominator = tf + k1 * (1 - b + b * (doc_len / avg_doc_len))
        score += idf * (numerator / denominator)

    return score


def bm25_search(query: str, news_data: list, top_k: int = 5, k1: float = None, b: float = None) -> list:
    """BM25 ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    if not news_data:
        return []

    # BM25 íŒŒë¼ë¯¸í„° (ì „ì—­ ìƒìˆ˜ ì‚¬ìš©)
    if k1 is None:
        k1 = BM25_K1 if 'BM25_K1' in globals() else 1.5
    if b is None:
        b = BM25_B if 'BM25_B' in globals() else 0.75

    # 1. ì¿¼ë¦¬ í† í°í™”
    query_tokens = tokenize(query)

    # 2. ëª¨ë“  ë‰´ìŠ¤ì˜ í…ìŠ¤íŠ¸ í† í°í™” (ì œëª© + ë‚´ìš©)
    news_tokens_list = [tokenize(news.title + " " + news.content) for news in news_data]

    # 3. ë¬¸ì„œ ë¹ˆë„ ê³„ì‚° (IDFìš©)
    doc_freqs = Counter()
    for tokens in news_tokens_list:
        for token in set(tokens):
            doc_freqs[token] += 1

    # 4. í‰ê·  ë¬¸ì„œ ê¸¸ì´ ê³„ì‚°
    avg_doc_len = sum(len(t) for t in news_tokens_list) / len(news_data)

    # 5. ê° ë‰´ìŠ¤ì— ëŒ€í•´ BM25 ìŠ¤ì½”ì–´ ê³„ì‚°
    results = []
    for news, doc_tokens in zip(news_data, news_tokens_list):
        score = bm25_score(query_tokens, doc_tokens, avg_doc_len, len(news_data), doc_freqs, k1=k1, b=b)
        results.append((news, score))

    # 6. ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ top_kê°œ ë°˜í™˜
    results.sort(key=lambda x: x[1], reverse=True)
    return results[:top_k]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Semantic Search
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def cosine_similarity(a: list, b: list) -> float:
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    a = np.array(a)
    b = np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def semantic_search(query: str, news_data: list, llm, top_k: int = 5) -> list:
    """ì„ë² ë”© ê¸°ë°˜ ì‹œë§¨í‹± ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    if not news_data:
        return []

    # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = llm.get_embedding(query)

    # 2. ê° ë‰´ìŠ¤ì™€ ìœ ì‚¬ë„ ê³„ì‚°
    results = []
    for news in news_data:
        if news.embedding:
            sim = cosine_similarity(query_embedding, news.embedding)
            results.append((news, sim))

    # 3. ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ top_kê°œ ë°˜í™˜
    results.sort(key=lambda x: x[1], reverse=True)
    return results[:top_k]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RAG íŒŒì´í”„ë¼ì¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_rag_answer(query: str, news_data: list, llm, use_semantic: bool = False, top_k: int = 3) -> dict:
    """RAG íŒŒì´í”„ë¼ì¸: ê²€ìƒ‰ + ìƒì„±"""

    # 1. ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰
    if use_semantic:
        relevant_news = semantic_search(query, news_data, llm, top_k=top_k)
    else:
        relevant_news = get_relevant_news(query, news_data, top_k=top_k)

    if not relevant_news:
        return {
            "answer": "ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "sources": []
        }

    # 2. ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    context = format_news_data(relevant_news)

    # 3. í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ë‹µë³€ ìƒì„±
    prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ê¸°ì‚¬:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""

    answer = llm.generate(prompt)

    return {
        "answer": answer,
        "sources": [(news.title, news.publisher, news.date) for news, _ in relevant_news]
    }


# === LLM í´ë˜ìŠ¤ ===
def get_secret(key: str):
    """API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (í™˜ê²½ë³€ìˆ˜ > Streamlit secrets)"""
    # í™˜ê²½ë³€ìˆ˜ ë¨¼ì € í™•ì¸
    value = os.getenv(key)
    if value:
        return value

    # Streamlit secrets í™•ì¸
    try:
        if hasattr(st, 'secrets') and key in st.secrets:
            return st.secrets[key]
    except Exception:
        pass

    return None


class GeminiLLM:
    def __init__(self, model: str = "gemini-2.0-flash"):
        from google import genai
        api_key = get_secret("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”")
        self.client = genai.Client(api_key=api_key)
        self.model = model
        self.embed_model = "text-embedding-004"

    def generate(self, prompt: str) -> str:
        response = self.client.models.generate_content(model=self.model, contents=prompt)
        return response.text

    def get_embedding(self, text: str) -> list:
        response = self.client.models.embed_content(model=self.embed_model, contents=text)
        return response.embeddings[0].values


class OpenAILLM:
    def __init__(self, model: str = "gpt-4o-mini", embedding_model: str = "text-embedding-3-small"):
        from openai import OpenAI
        api_key = get_secret("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”")
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.embedding_model = embedding_model

    def generate(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

    def get_embedding(self, text: str) -> list:
        response = self.client.embeddings.create(model=self.embedding_model, input=text)
        return response.data[0].embedding


def create_llm():
    """LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (GOOGLE_API_KEY ìš°ì„ )"""
    if get_secret("GOOGLE_API_KEY"):
        st.sidebar.success("âœ… Gemini API ì‚¬ìš©")
        return GeminiLLM()
    elif get_secret("OPENAI_API_KEY"):
        st.sidebar.success("âœ… OpenAI API ì‚¬ìš©")
        return OpenAILLM()
    else:
        raise ValueError("API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš” (GOOGLE_API_KEY ë˜ëŠ” OPENAI_API_KEY)")


# === RAG ì„¤ì • (ê³ ì •ê°’) ===
MAX_NEWS_ITEMS = 1000  # ë¡œë“œí•  ë‰´ìŠ¤ ìˆ˜
TOP_K_RESULTS = 3      # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
BM25_K1 = 1.5          # BM25 íŒŒë¼ë¯¸í„°
BM25_B = 0.75          # BM25 íŒŒë¼ë¯¸í„°

# === ì‚¬ì´ë“œë°” ===
with st.sidebar:
    st.header("ğŸ“° ì‹¬ë¦¬ ë‰´ìŠ¤ RAG")

    st.divider()

    # RAG íŒŒë¼ë¯¸í„° í‘œì‹œ (ì°¸ì¡°ìš©, ìˆ˜ì • ë¶ˆê°€)
    with st.expander("âš™ï¸ RAG íŒŒë¼ë¯¸í„° (ì°¸ì¡°)", expanded=False):
        st.markdown(f"""
        | íŒŒë¼ë¯¸í„° | ê°’ |
        |---------|-----|
        | ë¡œë“œ ë‰´ìŠ¤ ìˆ˜ | **{MAX_NEWS_ITEMS}** |
        | ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (top_k) | **{TOP_K_RESULTS}** |
        | BM25 k1 | **{BM25_K1}** |
        | BM25 b | **{BM25_B}** |
        """)

    if st.button("ğŸš€ ë°ì´í„° ë¡œë“œ", type="primary", use_container_width=True):
        with st.spinner("ì´ˆê¸°í™” ì¤‘..."):
            try:
                # ë°ì´í„° ë¡œë“œ (1000ê°œ ê³ ì •)
                news_data = load_news_data(max_items=MAX_NEWS_ITEMS)
                st.session_state.news_data = news_data

                # LLM ì´ˆê¸°í™”
                llm = create_llm()
                st.session_state.llm = llm

                st.success(f"âœ… {len(news_data)}ê°œ ë‰´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")

            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜: {e}")

    st.divider()

    # ê²€ìƒ‰ ë°©ì‹ ì„ íƒ
    search_method = st.radio(
        "ê²€ìƒ‰ ë°©ì‹",
        ["BM25 (í‚¤ì›Œë“œ)", "Semantic (ì„ë² ë”©)"],
        index=0
    )

    # ì„ë² ë”© ìƒì„± (Semantic Searchìš©)
    if st.session_state.news_data and st.session_state.llm:
        if search_method == "Semantic (ì„ë² ë”©)" and not st.session_state.embeddings_ready:
            if st.button("ğŸ§  ì„ë² ë”© ìƒì„±", use_container_width=True):
                with st.spinner("ì„ë² ë”© ìƒì„± ì¤‘..."):
                    progress = st.progress(0)
                    for i, news in enumerate(st.session_state.news_data):
                        text = news.title + " " + news.content[:500]
                        news.embedding = st.session_state.llm.get_embedding(text)
                        progress.progress((i + 1) / len(st.session_state.news_data))
                    st.session_state.embeddings_ready = True
                    st.success("âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!")

    st.divider()

    # ë²„íŠ¼ ì˜ì—­
    col1, col2 = st.columns(2)
    with col1:
        st.button("ğŸ”„ ìƒˆ ëŒ€í™”", on_click=reset_chat, use_container_width=True)
    with col2:
        st.button("ğŸ—‘ï¸ ì „ì²´ ì´ˆê¸°í™”", on_click=reset_all, use_container_width=True)

    # ìƒíƒœ í‘œì‹œ
    if st.session_state.news_data:
        st.success(f"ğŸ“š {len(st.session_state.news_data)}ê°œ ë‰´ìŠ¤ ì¤€ë¹„ë¨")
        if st.session_state.embeddings_ready:
            st.success("ğŸ§  ì„ë² ë”© ì¤€ë¹„ë¨")


# === ë©”ì¸ ì˜ì—­ ===
st.title("ğŸ“° RAG ì±—ë´‡ - ì‹¬ë¦¬ ë‰´ìŠ¤")

st.markdown("""
ì‹¬ë¦¬ ê´€ë ¨ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.

**ì˜ˆì‹œ ì§ˆë¬¸:**
- "ì •ì‹ ê±´ê°• ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ëŠ”?"
- "ì‹¬ë¦¬ìƒë‹´ íŠ¸ë Œë“œëŠ”?"
- "ìš°ìš¸ì¦ ì¹˜ë£Œ ê´€ë ¨ ë‰´ìŠ¤"
- "ì²­ì†Œë…„ ì‹¬ë¦¬ ë¬¸ì œ"
- "ì§ì¥ì¸ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë ¨ ê¸°ì‚¬"
""")

st.divider()

if not st.session_state.news_data:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ 'ë°ì´í„° ë¡œë“œ'ë¥¼ í´ë¦­í•˜ì„¸ìš”.")
else:
    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    with st.expander("ğŸ“Š ë¡œë“œëœ ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°"):
        for i, news in enumerate(st.session_state.news_data[:5]):
            st.markdown(f"**{i+1}. {news.title}**")
            st.caption(f"{news.publisher} | {news.date}")
            st.markdown(f"ğŸ“„ **ë³¸ë¬¸:**")
            st.write(news.content)
            st.divider()

    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"], avatar=msg.get("avatar")):
            st.markdown(msg["content"])
            if msg.get("sources"):
                with st.expander("ğŸ“š ì°¸ì¡° ë‰´ìŠ¤"):
                    for title, publisher, date in msg["sources"]:
                        st.markdown(f"**{title}** ({publisher}, {date})")

    # ì‚¬ìš©ì ì…ë ¥
    if user_input := st.chat_input("ì‹¬ë¦¬ ê´€ë ¨ ë‰´ìŠ¤ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”"):
        st.session_state.messages.append({
            "role": "user",
            "content": user_input,
            "avatar": AVATAR_USER
        })
        with st.chat_message("user", avatar=AVATAR_USER):
            st.markdown(user_input)

        with st.chat_message("assistant", avatar=AVATAR_BOT):
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                try:
                    use_semantic = (search_method == "Semantic (ì„ë² ë”©)" and st.session_state.embeddings_ready)

                    result = generate_rag_answer(
                        user_input,
                        st.session_state.news_data,
                        st.session_state.llm,
                        use_semantic=use_semantic,
                        top_k=TOP_K_RESULTS
                    )
                    response = result["answer"]
                    sources = result["sources"]

                    st.markdown(response)

                    if sources:
                        with st.expander("ğŸ“š ì°¸ì¡° ë‰´ìŠ¤"):
                            for title, publisher, date in sources:
                                st.markdown(f"**{title}** ({publisher}, {date})")

                except Exception as e:
                    response = f"âš ï¸ ì˜¤ë¥˜: {str(e)}"
                    sources = []
                    st.error(response)

        st.session_state.messages.append({
            "role": "assistant",
            "content": response,
            "avatar": AVATAR_BOT,
            "sources": sources
        })

        st.rerun()
