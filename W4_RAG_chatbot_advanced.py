"""
RAG ì±—ë´‡ - ì‹¬ë¦¬ ë‰´ìŠ¤ ê²€ìƒ‰ (Hybrid Search í¬í•¨)
========================================================

ì‹¬ë¦¬/ë‡Œê³¼í•™ ê´€ë ¨ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” RAG ì±—ë´‡
BM25, Semantic, Hybrid ê²€ìƒ‰ ë°©ì‹ ì§€ì›
"""

import streamlit as st
import pandas as pd
import numpy as np
import re
import math
import os
from pathlib import Path
from collections import Counter
from dataclasses import dataclass
from typing import Optional

# === í˜ì´ì§€ ì„¤ì • ===
st.set_page_config(
    page_title="RAG ì±—ë´‡ - Hybrid Search",
    page_icon="ğŸ“°",
    layout="wide"
)

# === ì„¤ì • ===
AVATAR_USER = "ğŸ‘¤"
AVATAR_BOT = "ğŸ¤–"
CSV_FILENAME = "Practice_data_NewsResult.CSV"
GITHUB_CSV_URL = "https://raw.githubusercontent.com/seonuan82/RAG_Workshop/main/Practice_data_NewsResult.CSV"


def get_data_path():
    """ë°ì´í„° íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (ë¡œì»¬ > í˜„ì¬ ë””ë ‰í† ë¦¬ > None)"""
    try:
        current_dir = Path(__file__).parent
        local_path = current_dir / CSV_FILENAME
        if local_path.exists():
            return str(local_path)
    except:
        pass

    cloud_path = Path(CSV_FILENAME)
    if cloud_path.exists():
        return str(cloud_path)

    return None


def load_news_from_github(max_items: int = 100) -> list:
    """GitHubì—ì„œ ë‰´ìŠ¤ CSV ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œë“œí•©ë‹ˆë‹¤."""
    import urllib.request
    import io

    st.info(f"ğŸ“¥ GitHubì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")

    try:
        with urllib.request.urlopen(GITHUB_CSV_URL) as response:
            content = response.read()

        st.info(f"ğŸ“¦ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(content):,} bytes")

        decoded = content.decode('cp949', errors='replace')
        df = pd.read_csv(io.StringIO(decoded))

        st.success(f"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ")
        st.info(f"ğŸ“Š ì´ {len(df)}ê°œ í–‰")

        return _parse_news_dataframe(df, max_items)

    except Exception as e:
        st.error(f"âŒ GitHub ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []


DATA_PATH = get_data_path()


# === ë°ì´í„° í´ë˜ìŠ¤ ===
@dataclass
class NewsItem:
    """ë‰´ìŠ¤ ë°ì´í„° í´ë˜ìŠ¤"""
    news_id: str
    date: str
    publisher: str
    title: str
    content: str
    url: str
    embedding: Optional[list] = None


# === ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ===
def init_session():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "news_data" not in st.session_state:
        st.session_state.news_data = []
    if "llm" not in st.session_state:
        st.session_state.llm = None
    if "embeddings_ready" not in st.session_state:
        st.session_state.embeddings_ready = False
    if "pending_response" not in st.session_state:
        st.session_state.pending_response = False
    if "search_method" not in st.session_state:
        st.session_state.search_method = "BM25"
    if "hybrid_alpha" not in st.session_state:
        st.session_state.hybrid_alpha = 0.5


init_session()


def reset_chat():
    """ëŒ€í™” ì´ˆê¸°í™”"""
    st.session_state.messages = []


def reset_all():
    """ì „ì²´ ì´ˆê¸°í™”"""
    st.session_state.messages = []
    st.session_state.news_data = []
    st.session_state.llm = None
    st.session_state.embeddings_ready = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _parse_news_dataframe(df: pd.DataFrame, max_items: int = 100) -> list:
    """DataFrameì„ NewsItem ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    news_list = []
    df = df.head(max_items)

    col_mapping = {
        'news_id': ['ë‰´ìŠ¤ ì‹ë³„ì', 'ê¸°ì‚¬ ê³ ìœ ë²ˆí˜¸', 'news_id', 'id'],
        'date': ['ì¼ì', 'date', 'ë‚ ì§œ'],
        'publisher': ['ì–¸ë¡ ì‚¬', 'publisher', 'ë§¤ì²´'],
        'title': ['ì œëª©', 'title'],
        'content': ['ë³¸ë¬¸', 'content', 'ë‚´ìš©'],
        'url': ['URL', 'url', 'ë§í¬']
    }

    def find_column(candidates):
        for col in candidates:
            if col in df.columns:
                return col
        return None

    id_col = find_column(col_mapping['news_id'])
    date_col = find_column(col_mapping['date'])
    publisher_col = find_column(col_mapping['publisher'])
    title_col = find_column(col_mapping['title'])
    content_col = find_column(col_mapping['content'])
    url_col = find_column(col_mapping['url'])

    if not title_col or not content_col:
        st.error(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    for _, row in df.iterrows():
        try:
            news = NewsItem(
                news_id=str(row.get(id_col, '')) if id_col else '',
                date=str(row.get(date_col, '')) if date_col else '',
                publisher=str(row.get(publisher_col, '')) if publisher_col else '',
                title=str(row.get(title_col, '')),
                content=str(row.get(content_col, '')),
                url=str(row.get(url_col, '')) if url_col else ''
            )
            news_list.append(news)
        except Exception:
            continue

    st.success(f"âœ… {len(news_list)}ê°œ ë‰´ìŠ¤ íŒŒì‹± ì™„ë£Œ")
    return news_list


def load_news_data(filepath: Optional[str] = None, max_items: int = 100) -> list:
    """GitHubì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    return load_news_from_github(max_items)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë‰´ìŠ¤ ë°ì´í„° í¬ë§·íŒ…
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def format_news_data(news_results: list) -> str:
    """ê²€ìƒ‰ëœ ë‰´ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    formatted_list = []

    for news, score in news_results:
        formatted = f"ì œëª©: {news.title}\nì–¸ë¡ ì‚¬: {news.publisher}\në‚ ì§œ: {news.date}\në³¸ë¬¸: {news.content}"
        formatted_list.append(formatted)

    return "\n\n---\n\n".join(formatted_list)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BM25 ê²€ìƒ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def tokenize(text: str) -> list:
    """ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì €"""
    text = text.lower()
    text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
    tokens = text.split()
    stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were',
                 'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'í•œ', 'ìˆë‹¤', 'í•˜ë‹¤'}
    return [t for t in tokens if t not in stopwords and len(t) > 1]


def bm25_score(query_tokens: list, doc_tokens: list,
               avg_doc_len: float, doc_count: int, doc_freqs: dict,
               k1: float = 1.5, b: float = 0.75) -> float:
    """BM25 ìŠ¤ì½”ì–´ ê³„ì‚°"""
    score = 0.0
    doc_len = len(doc_tokens)
    doc_token_counts = Counter(doc_tokens)

    for token in query_tokens:
        if token not in doc_token_counts:
            continue
        tf = doc_token_counts[token]
        df = doc_freqs.get(token, 0)
        idf = math.log((doc_count - df + 0.5) / (df + 0.5) + 1)
        numerator = tf * (k1 + 1)
        denominator = tf + k1 * (1 - b + b * (doc_len / avg_doc_len))
        score += idf * (numerator / denominator)

    return score


def bm25_search(query: str, news_data: list, top_k: int = 5, k1: float = None, b: float = None) -> list:
    """BM25 ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    if not news_data:
        return []

    if k1 is None:
        k1 = BM25_K1 if 'BM25_K1' in globals() else 1.5
    if b is None:
        b = BM25_B if 'BM25_B' in globals() else 0.75

    # 1. ì¿¼ë¦¬ í† í°í™”
    query_tokens = tokenize(query)

    # 2. ëª¨ë“  ë‰´ìŠ¤ì˜ í…ìŠ¤íŠ¸ í† í°í™” (ì œëª© + ë‚´ìš©)
    news_tokens_list = [tokenize(news.title + " " + news.content) for news in news_data]

    # 3. ë¬¸ì„œ ë¹ˆë„ ê³„ì‚° (IDFìš©)
    doc_freqs = Counter()
    for tokens in news_tokens_list:
        for token in set(tokens):
            doc_freqs[token] += 1

    # 4. í‰ê·  ë¬¸ì„œ ê¸¸ì´ ê³„ì‚°
    avg_doc_len = sum(len(t) for t in news_tokens_list) / len(news_data)

    # 5. ê° ë‰´ìŠ¤ì— ëŒ€í•´ BM25 ìŠ¤ì½”ì–´ ê³„ì‚°
    results = []
    for news, doc_tokens in zip(news_data, news_tokens_list):
        score = bm25_score(query_tokens, doc_tokens, avg_doc_len, len(news_data), doc_freqs, k1=k1, b=b)
        results.append((news, score))

    # 6. ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ top_kê°œ ë°˜í™˜
    results.sort(key=lambda x: x[1], reverse=True)
    return results[:top_k]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Semantic Search
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def cosine_similarity(a: list, b: list) -> float:
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    a = np.array(a)
    b = np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def semantic_search(query: str, news_data: list, llm, top_k: int = 5) -> list:
    """ì„ë² ë”© ê¸°ë°˜ ì‹œë§¨í‹± ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    if not news_data:
        return []

    # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = llm.get_embedding(query)

    # 2. ê° ë‰´ìŠ¤ì™€ ìœ ì‚¬ë„ ê³„ì‚°
    results = []
    for news in news_data:
        if news.embedding:
            sim = cosine_similarity(query_embedding, news.embedding)
            results.append((news, sim))

    # 3. ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ top_kê°œ ë°˜í™˜
    results.sort(key=lambda x: x[1], reverse=True)
    return results[:top_k]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Hybrid Search (NEW!)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def normalize_scores(results: list) -> list:
    """
    ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. (Min-Max Normalization)

    Args:
        results: [(NewsItem, score), ...] ë¦¬ìŠ¤íŠ¸

    Returns:
        [(NewsItem, normalized_score), ...] ë¦¬ìŠ¤íŠ¸
    """
    if not results:
        return []

    scores = [score for _, score in results]
    min_score = min(scores)
    max_score = max(scores)

    # ëª¨ë“  ì ìˆ˜ê°€ ê°™ìœ¼ë©´ 1ë¡œ ì„¤ì •
    if max_score == min_score:
        return [(news, 1.0) for news, _ in results]

    normalized = []
    for news, score in results:
        norm_score = (score - min_score) / (max_score - min_score)
        normalized.append((news, norm_score))

    return normalized


def hybrid_search(query: str, news_data: list, llm, top_k: int = 5, alpha: float = 0.5) -> list:
    """
    BM25ì™€ Semantic Searchë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        news_data: NewsItem ë¦¬ìŠ¤íŠ¸
        llm: LLM ì¸ìŠ¤í„´ìŠ¤
        top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        alpha: BM25 ê°€ì¤‘ì¹˜ (0~1). 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ BM25 ì¤‘ì‹¬, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ Semantic ì¤‘ì‹¬

    Returns:
        [(NewsItem, score), ...] ë¦¬ìŠ¤íŠ¸

    ğŸ“Š Alpha ê°’ì— ë”°ë¥¸ íŠ¹ì„±:
    - alpha = 1.0: BM25ë§Œ ì‚¬ìš© (í‚¤ì›Œë“œ ë§¤ì¹­ ì¤‘ì‹¬)
    - alpha = 0.5: ê· í˜• ìˆëŠ” í•˜ì´ë¸Œë¦¬ë“œ
    - alpha = 0.0: Semanticë§Œ ì‚¬ìš© (ì˜ë¯¸ ìœ ì‚¬ë„ ì¤‘ì‹¬)
    """
    if not news_data:
        return []

    # 1. BM25 ê²€ìƒ‰ ìˆ˜í–‰ (ì „ì²´ ë¬¸ì„œì— ëŒ€í•´)
    bm25_results = bm25_search(query, news_data, top_k=len(news_data))
    bm25_normalized = normalize_scores(bm25_results)

    # 2. Semantic ê²€ìƒ‰ ìˆ˜í–‰ (ì „ì²´ ë¬¸ì„œì— ëŒ€í•´)
    semantic_results = semantic_search(query, news_data, llm, top_k=len(news_data))
    semantic_normalized = normalize_scores(semantic_results)

    # 3. ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    bm25_scores = {news.news_id: score for news, score in bm25_normalized}
    semantic_scores = {news.news_id: score for news, score in semantic_normalized}

    # 4. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
    hybrid_results = []
    for news in news_data:
        bm25_s = bm25_scores.get(news.news_id, 0)
        sem_s = semantic_scores.get(news.news_id, 0)
        final_score = alpha * bm25_s + (1 - alpha) * sem_s
        hybrid_results.append((news, final_score))

    # 5. ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ top_kê°œ ë°˜í™˜
    hybrid_results.sort(key=lambda x: x[1], reverse=True)
    return hybrid_results[:top_k]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ê´€ë ¨ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ê²€ìƒ‰ ë°©ì‹ í†µí•©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_relevant_news(query: str, news_data: list, llm=None, top_k: int = 5,
                      search_method: str = "BM25", alpha: float = 0.5) -> list:
    """
    ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        news_data: NewsItem ë¦¬ìŠ¤íŠ¸
        llm: LLM ì¸ìŠ¤í„´ìŠ¤ (Semantic/Hybrid ê²€ìƒ‰ ì‹œ í•„ìš”)
        top_k: ë°˜í™˜í•  ë‰´ìŠ¤ ìˆ˜
        search_method: ê²€ìƒ‰ ë°©ì‹ ("BM25", "Semantic", "Hybrid")
        alpha: Hybrid ê²€ìƒ‰ ì‹œ BM25 ê°€ì¤‘ì¹˜

    Returns:
        ê´€ë ¨ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ [(NewsItem, score), ...]
    """
    if search_method == "BM25":
        return bm25_search(query, news_data, top_k)
    elif search_method == "Semantic":
        return semantic_search(query, news_data, llm, top_k)
    elif search_method == "Hybrid":
        return hybrid_search(query, news_data, llm, top_k, alpha)
    else:
        return bm25_search(query, news_data, top_k)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RAG íŒŒì´í”„ë¼ì¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_rag_answer(query: str, news_data: list, llm,
                        search_method: str = "BM25", alpha: float = 0.5, top_k: int = 3) -> dict:
    """RAG íŒŒì´í”„ë¼ì¸: ê²€ìƒ‰ + ìƒì„±"""

    # 1. ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰
    relevant_news = get_relevant_news(
        query, news_data, llm,
        top_k=top_k,
        search_method=search_method,
        alpha=alpha
    )

    if not relevant_news:
        return {
            "answer": "ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "sources": [],
            "search_method": search_method
        }

    # 2. ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    context = format_news_data(relevant_news)

    # 3. í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ë‹µë³€ ìƒì„±
    prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ê¸°ì‚¬:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""

    answer = llm.generate(prompt)

    return {
        "answer": answer,
        "sources": [(news.title, news.publisher, news.date) for news, _ in relevant_news],
        "search_method": search_method
    }


# === LLM í´ë˜ìŠ¤ ===
def get_secret(key: str):
    """API í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (í™˜ê²½ë³€ìˆ˜ > Streamlit secrets)"""
    value = os.getenv(key)
    if value:
        return value

    try:
        if hasattr(st, 'secrets') and key in st.secrets:
            return st.secrets[key]
    except Exception:
        pass

    return None


class GeminiLLM:
    def __init__(self, model: str = "gemini-2.5-flash"):
        from google import genai
        api_key = get_secret("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”")
        self.client = genai.Client(api_key=api_key)
        self.model = model
        self.embed_model = "text-embedding-004"

    def generate(self, prompt: str) -> str:
        response = self.client.models.generate_content(model=self.model, contents=prompt)
        return response.text

    def get_embedding(self, text: str) -> list:
        response = self.client.models.embed_content(model=self.embed_model, contents=text)
        return response.embeddings[0].values


class OpenAILLM:
    def __init__(self, model: str = "gpt-4o-mini", embedding_model: str = "text-embedding-3-small"):
        from openai import OpenAI
        api_key = get_secret("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”")
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.embedding_model = embedding_model

    def generate(self, prompt: str) -> str:
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

    def get_embedding(self, text: str) -> list:
        response = self.client.embeddings.create(model=self.embedding_model, input=text)
        return response.data[0].embedding


def create_llm():
    """LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (GOOGLE_API_KEY ìš°ì„ )"""
    if get_secret("GOOGLE_API_KEY"):
        st.sidebar.success("âœ… Gemini API ì‚¬ìš©")
        return GeminiLLM()
    elif get_secret("OPENAI_API_KEY"):
        st.sidebar.success("âœ… OpenAI API ì‚¬ìš©")
        return OpenAILLM()
    else:
        raise ValueError("API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš” (GOOGLE_API_KEY ë˜ëŠ” OPENAI_API_KEY)")


# === RAG ì„¤ì • (ê³ ì •ê°’) ===
MAX_NEWS_ITEMS = 1000  # ë¡œë“œí•  ë‰´ìŠ¤ ìˆ˜
TOP_K_RESULTS = 3      # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
BM25_K1 = 1.5          # BM25 íŒŒë¼ë¯¸í„°
BM25_B = 0.75          # BM25 íŒŒë¼ë¯¸í„°

# === ì‚¬ì´ë“œë°” ===
with st.sidebar:
    st.header("ğŸ“° ì‹¬ë¦¬ ë‰´ìŠ¤ RAG")
    st.caption("Hybrid Search ì§€ì›")

    st.divider()

    # RAG íŒŒë¼ë¯¸í„° í‘œì‹œ
    with st.expander("âš™ï¸ RAG íŒŒë¼ë¯¸í„° (ì°¸ì¡°)", expanded=False):
        st.markdown(f"""
        | íŒŒë¼ë¯¸í„° | ê°’ |
        |---------|-----|
        | ë¡œë“œ ë‰´ìŠ¤ ìˆ˜ | **{MAX_NEWS_ITEMS}** |
        | ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (top_k) | **{TOP_K_RESULTS}** |
        | BM25 k1 | **{BM25_K1}** |
        | BM25 b | **{BM25_B}** |
        """)

    if st.button("ğŸš€ ë°ì´í„° ë¡œë“œ", type="primary", use_container_width=True):
        with st.spinner("ì´ˆê¸°í™” ì¤‘..."):
            try:
                news_data = load_news_data(max_items=MAX_NEWS_ITEMS)
                st.session_state.news_data = news_data

                llm = create_llm()
                st.session_state.llm = llm

                st.success(f"âœ… {len(news_data)}ê°œ ë‰´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")

            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜: {e}")

    st.divider()

    # ê²€ìƒ‰ ë°©ì‹ ì„ íƒ
    st.subheader("ğŸ” ê²€ìƒ‰ ë°©ì‹")
    search_method = st.radio(
        "ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜",
        ["BM25", "Semantic", "Hybrid"],
        index=0,
        help="BM25: í‚¤ì›Œë“œ ê¸°ë°˜, Semantic: ì˜ë¯¸ ê¸°ë°˜, Hybrid: ë‘˜ì˜ ì¡°í•©"
    )
    st.session_state.search_method = search_method

    # Hybrid ê²€ìƒ‰ ì‹œ alpha ê°’ ì¡°ì ˆ
    if search_method == "Hybrid":
        alpha = st.slider(
            "Alpha (BM25 ê°€ì¤‘ì¹˜)",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.1,
            help="1.0: BM25ë§Œ, 0.0: Semanticë§Œ, 0.5: ê· í˜•"
        )
        st.session_state.hybrid_alpha = alpha
        st.caption(f"ğŸ“Š BM25: {alpha:.0%} / Semantic: {1-alpha:.0%}")

    st.divider()

    # ì„ë² ë”© ìƒì„± (Semantic/Hybridìš©)
    if st.session_state.news_data and st.session_state.llm:
        if search_method in ["Semantic", "Hybrid"] and not st.session_state.embeddings_ready:
            st.warning("âš ï¸ ì„ë² ë”©ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            if st.button("ğŸ§  ì„ë² ë”© ìƒì„±", use_container_width=True):
                with st.spinner("ì„ë² ë”© ìƒì„± ì¤‘..."):
                    progress = st.progress(0)
                    for i, news in enumerate(st.session_state.news_data):
                        text = news.title + " " + news.content[:500]
                        news.embedding = st.session_state.llm.get_embedding(text)
                        progress.progress((i + 1) / len(st.session_state.news_data))
                    st.session_state.embeddings_ready = True
                    st.success("âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!")

    st.divider()

    # ë²„íŠ¼ ì˜ì—­
    col1, col2 = st.columns(2)
    with col1:
        st.button("ğŸ”„ ìƒˆ ëŒ€í™”", on_click=reset_chat, use_container_width=True)
    with col2:
        st.button("ğŸ—‘ï¸ ì „ì²´ ì´ˆê¸°í™”", on_click=reset_all, use_container_width=True)

    # ìƒíƒœ í‘œì‹œ
    if st.session_state.news_data:
        st.success(f"ğŸ“š {len(st.session_state.news_data)}ê°œ ë‰´ìŠ¤ ì¤€ë¹„ë¨")
        if st.session_state.embeddings_ready:
            st.success("ğŸ§  ì„ë² ë”© ì¤€ë¹„ë¨")


# === ë©”ì¸ ì˜ì—­ ===
st.title("ğŸ“° RAG ì±—ë´‡ - Hybrid Search")

st.markdown("ì‹¬ë¦¬ ê´€ë ¨ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤."}
md1, md2 = st.columns(2)
with md1:
    st.markdown("""
#### ğŸ” ê²€ìƒ‰ ë°©ì‹ ë¹„êµ:
| ë°©ì‹ | ì¥ì  | ë‹¨ì  |
|------|------|------|
| **BM25** | ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­, ë¹ ë¦„ | ë™ì˜ì–´/ìœ ì‚¬ì–´ ì¸ì‹ ëª»í•¨ |
| **Semantic** | ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒŒì•… | í‚¤ì›Œë“œ ì •í™•ë„ ë‚®ìŒ, ëŠë¦¼ |
| **Hybrid** | ë‘ ì¥ì  ê²°í•© | íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš” |
""")
with md2:
    st.markdown("""
**ì˜ˆì‹œ ì§ˆë¬¸:**
- "ì •ì‹ ê±´ê°• ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ëŠ”?"
- "ê²½ì œì‹¬ë¦¬ ê´€ë ¨ ë‰´ìŠ¤ëŠ”?"
- "ê³µì§ì ëŒ€ìƒ ì •ì‹ ê±´ê°•"
- "ì²­ì†Œë…„ ì‹¬ë¦¬ ë¬¸ì œ"
- "ì§ì¥ì¸ ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë ¨ ê¸°ì‚¬"
""")

st.divider()

if not st.session_state.news_data:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ 'ë°ì´í„° ë¡œë“œ'ë¥¼ í´ë¦­í•˜ì„¸ìš”.")
else:
    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    with st.expander("ğŸ“Š ë¡œë“œëœ ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°"):
        for i, news in enumerate(st.session_state.news_data[:5]):
            st.markdown(f"**{i+1}. {news.title}**")
            st.caption(f"{news.publisher} | {news.date}")
            st.write(news.content[:200] + "...")
            st.divider()

    # í˜„ì¬ ê²€ìƒ‰ ë°©ì‹ í‘œì‹œ
    method_emoji = {"BM25": "ğŸ”¤", "Semantic": "ğŸ§ ", "Hybrid": "âš¡"}
    alpha_info = f" (Alpha: {st.session_state.hybrid_alpha})" if st.session_state.search_method == "Hybrid" else ""
    st.info(f"{method_emoji.get(st.session_state.search_method, '')} í˜„ì¬ ê²€ìƒ‰ ë°©ì‹: **{st.session_state.search_method}**{alpha_info}")

    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ (ìŠ¤í¬ë¡¤ ê°€ëŠ¥í•œ ì»¨í…Œì´ë„ˆ)
    chat_container = st.container(height=500)
    with chat_container:
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"], avatar=msg.get("avatar")):
                st.markdown(msg["content"])
                if msg.get("sources"):
                    with st.expander(f"ğŸ“š ì°¸ì¡° ë‰´ìŠ¤ ({msg.get('search_method', 'N/A')})"):
                        for title, publisher, date in msg["sources"]:
                            st.markdown(f"**{title}** ({publisher}, {date})")

        # ì‘ë‹µ ìƒì„± ì¤‘ì¸ ê²½ìš° (ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ ì²˜ë¦¬)
        if st.session_state.pending_response:
            last_user_input = st.session_state.messages[-1]["content"]
            with st.chat_message("assistant", avatar=AVATAR_BOT):
                with st.spinner(f"ë‹µë³€ ìƒì„± ì¤‘... ({st.session_state.search_method})"):
                    try:
                        result = generate_rag_answer(
                            last_user_input,
                            st.session_state.news_data,
                            st.session_state.llm,
                            search_method=st.session_state.search_method,
                            alpha=st.session_state.hybrid_alpha,
                            top_k=TOP_K_RESULTS
                        )
                        response = result["answer"]
                        sources = result["sources"]
                        used_method = result.get("search_method", st.session_state.search_method)

                    except Exception as e:
                        response = f"âš ï¸ ì˜¤ë¥˜: {str(e)}"
                        sources = []
                        used_method = st.session_state.search_method

            st.session_state.messages.append({
                "role": "assistant",
                "content": response,
                "avatar": AVATAR_BOT,
                "sources": sources,
                "search_method": used_method
            })
            st.session_state.pending_response = False
            st.rerun()

    # ìë™ ìŠ¤í¬ë¡¤
    if st.session_state.messages:
        st.components.v1.html(
            """
            <script>
                const chatContainers = window.parent.document.querySelectorAll('[data-testid="stVerticalBlockBorderWrapper"]');
                chatContainers.forEach(container => {
                    const scrollable = container.querySelector('[data-testid="stVerticalBlock"]');
                    if (scrollable && scrollable.scrollHeight > scrollable.clientHeight) {
                        scrollable.scrollTop = scrollable.scrollHeight;
                    }
                });
            </script>
            """,
            height=0
        )

    # ì‚¬ìš©ì ì…ë ¥
    if user_input := st.chat_input("ì§ˆë¬¸í•˜ì„¸ìš”"):
        st.session_state.messages.append({
            "role": "user",
            "content": user_input,
            "avatar": AVATAR_USER
        })
        st.session_state.pending_response = True
        st.rerun()

